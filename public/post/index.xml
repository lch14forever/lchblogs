<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Chenhao&#39;s Personal Page</title>
    <link>/post/</link>
    <description>Recent content in Posts on Chenhao&#39;s Personal Page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© This site was created with R BlogDown and HuGo by Chenhao Li.</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>圆角barplot</title>
      <link>/post/2022-05-32-r-round-corner/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-32-r-round-corner/</guid>
      <description>依赖 library(dplyr) ## ## Attaching package: &amp;#39;dplyr&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag ## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union library(ggplot2)  数据 dat &amp;lt;- mutate(mtcars, index=1:n()) dat ## mpg cyl disp hp drat wt qsec vs am gear carb index ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 ## Mazda RX4 Wag 21.</description>
    </item>
    
    <item>
      <title>让Single cell UMAP注释支棱起来</title>
      <link>/post/2021-08-12-singlecell-annotation-ggforce/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-08-12-singlecell-annotation-ggforce/</guid>
      <description>最近在画UMAP的时候发现有的时候细胞亚群的注释与点重合颜色上不是很搭配，同事提出让注释“支棱”起来，首先想到的是ggforce中的geom_mark_ellipse，实践中遇到一些问题，于是有了第一篇Single cell的记录。
ggforee
 尝试用ggforce注释 library(dplyr) library(Seurat) library(SeuratData) library(patchwork) library(ggforce) ##InstallData(&amp;quot;pbmc3k&amp;quot;) data(&amp;quot;pbmc3k&amp;quot;) points &amp;lt;- data.frame(pbmc3k.final@reductions$umap@cell.embeddings, cluster=Idents(pbmc3k.final)) DimPlot(pbmc3k.final) + geom_mark_ellipse(data=points, aes(x=UMAP_1, y=UMAP_2, label=cluster, col=cluster), inherit.aes = F) + NoLegend() 非常难看不是吗？因为有一些cluster（Naive CD4 T）存在异常值，ggforce中的函数会包含所有的点。所以应该将异常值去掉，这个方法有很多，我使用的是之前用到的置信椭圆的方法。
 修改 思路如下：
 对每一个cluster计算一个尽量小的置信椭圆 用置信椭圆上的点来画geom_mark_ellipse  points &amp;lt;- data.frame(pbmc3k.final@reductions$umap@cell.embeddings, cluster=Idents(pbmc3k.final)) ## adapted from https://github.com/fawda123/ggord/blob/master/R/ggord.R theta &amp;lt;- c(seq(-pi, pi, length = 50), seq(pi, -pi, length = 50)) circle &amp;lt;- cbind(cos(theta), sin(theta)) library(plyr) ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems.</description>
    </item>
    
    <item>
      <title>Nextflow长命令segmentation fault的解决</title>
      <link>/post/2021-08-08-nextflow-trick/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-08-08-nextflow-trick/</guid>
      <description>  最近在Mac上运行nextflow的一个流程，流程下载上千个文件，然后其中一步涉及将这上千个文件拷贝到一个文件夹下，运行的命令大概如下
cp d0001.rds d0002.rds d0003.rds ... d1000.rds data_rds/ 在nextflow的流程中这样实现
process gatherRDS { input: file rds output: path &amp;quot;data&amp;quot; script: &amp;quot;&amp;quot;&amp;quot; mkdir data cp $rds data/ &amp;quot;&amp;quot;&amp;quot; } 但是运行时不断遇到segmentation fault的情况（MacOS Big Sur 11.5）。在github找到了答案。加入ulimit -Ss unlimited，可以解除对程序的内存限制，解决问题。
process gatherRDS { beforeScript &amp;#39;ulimit -Ss unlimited&amp;#39; // avoid long input segfault input: file rds output: path &amp;quot;data&amp;quot; script: &amp;quot;&amp;quot;&amp;quot; mkdir data cp $rds data/ &amp;quot;&amp;quot;&amp;quot; } </description>
    </item>
    
    <item>
      <title>nf-core流程使用</title>
      <link>/post/2021-05-20-nextflow-nfcore/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-05-20-nextflow-nfcore/</guid>
      <description>nf-core是由Nextflow使用开发者共同开发维护的优秀项目，涵盖各种生信数据分析的高质量流程。依赖于Nextflow的优势，基本可以在任何HPC环境即插即用，解决生信分析中规模化、可重复性等众多痛点。本文以ATAC-Seq为例，介绍如何使用nf-core中的流程。该流程从质控、比对到差异分析、IGV可视化一键完成。
测试环境 Ubuntu 20.04 （Root权限）
安装Docker
sudo snap install docker # 用snap安装 sudo usermod -aG docker ${USER} # 修正权限问题 安装java
sudo apt install default-jdk 安装nextflow
curl -fsSL get.nextflow.io | bash sudo mv nextflow /usr/local/bin 下载nf-core/atacseq流程并运行实例
 nextflow run nf-core/atacseq -profile test,docker  运行 根据自己的样本，创建一个以下格式的csv文件
group,replicate,fastq_1,fastq_2 control,1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz control,2,AEG588A2_S2_L002_R1_001.fastq.gz,AEG588A2_S2_L002_R2_001.fastq.gz control,3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz treatment,1,AEG588A4_S4_L003_R1_001.fastq.gz,AEG588A4_S4_L003_R2_001.fastq.gz treatment,2,AEG588A5_S5_L003_R1_001.fastq.gz,AEG588A5_S5_L003_R2_001.fastq.gz treatment,3,AEG588A6_S6_L003_R1_001.fastq.gz,AEG588A6_S6_L003_R2_001.fastq.gz treatment,3,AEG588A6_S6_L004_R1_001.fastq.gz,AEG588A6_S6_L004_R2_001.fastq.gz  group：实验组别，如实验组、对照组 replicate：实验重复编号 fastq_1：fastq1绝对路径 fastq_2：fastq2绝对路径  运行
export NXF_OPTS=&amp;#39;-Xms1g -Xmx4g&amp;#39; # 建议限制Java虚拟机的内存 nextflow run nf-core/atacseq --input design.</description>
    </item>
    
    <item>
      <title>Telegram bot基础使用</title>
      <link>/post/2021-05-11-telegrambot/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-05-11-telegrambot/</guid>
      <description>申请telegram bot 在telegram中搜索“BotFather”，进入与其聊天页面 输入/start查看可用选项 输入或点击/newbot 输入bot的名字（必须以Bot或_bot结尾）  完成后收到如下消息，点击t.me/xxxxxx_bot即可进入与bot聊天页面
Done! Congratulations on your new bot. You will find it at t.me/xxxxxx_bot. You can now add a description, about section and profile picture for your bot, see /help for a list of commands. By the way, when you’ve finished creating your cool bot, ping our Bot Support if you want a better username for it. Just make sure the bot is fully operational before you do this.</description>
    </item>
    
    <item>
      <title>Kraken拓展工具KrakenTools</title>
      <link>/post/2020-08-22-krakentools/</link>
      <pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-08-22-krakentools/</guid>
      <description>背景 Kraken（Kraken2）默认的report格式并不利于后续的分析，在运行Kraken时我通常会使用--use-mpa-style这个参数来生成像MetaPhlan（MetaPhlan2）格式的结果。但是如果想要做后续的分析（Bracken），就还要用到report格式的结果。之前我采用的策略是运行两次Kraken2：
### run it twice... kraken2 \ --db $KRAKEN_DB \ --paired \ --threads 8 \ --output ${prefix}.out \ --report ${prefix}.kraken2.tsv \ $fq1 $fq2 \ --use-mpa-style ### run again for bracken kraken2 \ --db $KRAKEN_DB \ --paired \ --threads 8 \ --report ${prefix}.kraken2 \ $fq1 $fq2 &amp;gt; /dev/null 经验上来讲Kraken很大一部分运行时间花在将数据库载入内存和硬盘读写（IO），对于大数据库、深度测序，会造成一定的资源浪费、并且CPU利用率不高。
最近发现了Bracken的作者开发的很有用的工具集。其中包含一个将Kraken report格式转换为mpa格式的输出的脚本kreport2mpa.py。
 使用kreport2mpa.py简化kraken流程 下面是简化后的Kraken2流程：
kraken2 \ --db $KRAKEN_DB \ --paired \ --threads 8 \ --output ${prefix}.kraken2.out \ --report ${prefix}.kraken2.report \ $reads1 $reads2 ### Convert kraken report to mpa file kreport2mpa.</description>
    </item>
    
    <item>
      <title>基于Nextflow的宏基因组有参分析-VI Nextflow tower本地配置</title>
      <link>/post/2019-10-29-nextflow-tower-deploy/</link>
      <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-10-29-nextflow-tower-deploy/</guid>
      <description>测试环境 Ubuntu 18.04 （Root权限）
AWS安全组配置：
  Custom TCP TCP 8000 0.0.0.0/0 ss    Custom TCP TCP 8000 ::/0 ss     安装jdk（&amp;gt;1.8）及编译依赖 sudo apt update sudo apt install openjdk-8-jdk-headless sudo apt install build-essential  安装docker，docker-compose sudo apt install docker.io sudo curl -L &amp;quot;https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)&amp;quot; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose  更改docker权限（此处需要登出并重新登陆） sudo usermod -a -G docker $USER  下载nf-tower ## install git sudo apt install git git clone https://github.</description>
    </item>
    
    <item>
      <title>基于Nextflow的宏基因组有参分析-V Nextflow tower</title>
      <link>/post/2019-10-25-nextflow-tower/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-10-25-nextflow-tower/</guid>
      <description>登录Nextflow Tower的官方网站
点击“Sign in”并输入邮箱，会在邮箱中收到登录链接
登陆后看到如下界面
在运行nextflow前设置环境变量
$ export TOWER_ACCESS_TOKEN=xxxxxxxxxxxxxxxxxxxxxxx 在运行上一篇的流程时，加入-with-tower参数
$ ./main.nf --read_path data -with-tower N E X T F L O W ~ version 19.09.0-edge Launching `./main.nf` [angry_venter] - revision: 72dddbcd1f WARN: DSL 2 IS AN EXPERIMENTAL FEATURE UNDER DEVELOPMENT -- SYNTAX MAY CHANGE IN FUTURE RELEASE Monitor the execution with Nextflow Tower using this url https://tower.nf/watch/xxxxxx executor &amp;gt; local (8)  使用上面的链接或登录Nextflow Tower便可实时监控流程的运行
运行完成后可以查看流程使用资源的情况
(未完)</description>
    </item>
    
    <item>
      <title>基于Nextflow的宏基因组有参分析-IV Kraken2&#43;Bracken</title>
      <link>/post/2019-10-17-nextflow-kraken/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-10-17-nextflow-kraken/</guid>
      <description>Shell脚本 和fastp+decont的步骤一样，我们可以首先将流程写成一个shell脚本：
#!/bin/bash set -e -o pipefail source activate metagenomics fq1=$1 fq2=$2 prefix=$3 KRAKEN_DB=/data/minikraken2_v2_8GB ### run it twice... kraken2 \ --db $KRAKEN_DB \ --paired \ --threads 8 \ --output ${prefix}.out \ --report ${prefix}.kraken2.tsv \ $fq1 $fq2 \ --use-mpa-style ### run again for bracken kraken2 \ --db $KRAKEN_DB \ --paired \ --threads 8 \ --report ${prefix}.kraken2 \ $fq1 $fq2 &amp;gt; /dev/null for tax in s g; do bracken -d ${KRAKEN_DB} \ -i ${prefix}.</description>
    </item>
    
    <item>
      <title>基于Nextflow的宏基因组有参分析-III 质控&#43;去宿主DNA</title>
      <link>/post/2019-10-01-nextflow-decont/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-10-01-nextflow-decont/</guid>
      <description>Shell脚本 质控和去宿主主要由以下工具完成：
fastp: 去接头，修剪低质量序列 BWA + samtools：去宿主  参照这篇教程，我们可以设计如下简单的流程：
#!/bin/bash ## requirement: ## fastp, BWA, samtools &amp;gt;= 1.7 ref=$1 reads1=$2 reads2=$3 prefix=$4 threads=${5:-4} fastp -i $reads1 -I $reads2 --stdout -j ${prefix}.json -h ${prefix}.html | bwa mem -p -t $threads $ref - | samtools fastq -f12 -F256 -1 ${prefix}_fastpdecont_1.fastq.gz -2 ${prefix}_fastpdecont_2.fastq.gz - 此处尽可能地使用管道来避免硬盘读写和不必要的数据存储。
 Nextflow模块 我们可以把以上脚本写成简单的nextflow模块decont.nf：
params.index = &amp;#39;hg19.fa&amp;#39; params.outdir = &amp;#39;./&amp;#39; process DECONT { tag &amp;quot;${prefix}&amp;quot; cpus 8 publishDir params.</description>
    </item>
    
    <item>
      <title>基于Nextflow的宏基因组有参分析-II 主函数和输入输出</title>
      <link>/post/2019-09-29-nextflow-main/</link>
      <pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-29-nextflow-main/</guid>
      <description>第一个Nextflow程序 创建以下名为main.nf的Nextflow文件
#!/usr/bin/env nextflow // DSL 2 syntax nextflow.preview.dsl=2 // parameters params.help = false params.read_path = &amp;quot;${workflow.projectDir}/data&amp;quot; // help message def helpMessage() { log.info&amp;quot;&amp;quot;&amp;quot; ================================================================= Usage: ${workflow.projectDir}/main.nf --read_path PATH/OF/READS ================================================================= &amp;quot;&amp;quot;&amp;quot;.stripIndent() } if (params.help){ helpMessage() exit 0 } // Create channel for reads ch_reads = Channel .fromFilePairs(params.read_path + &amp;#39;/**{1,2}.f*q*&amp;#39;, flat: true) ch_reads.view()  逐行解析 Shebang #!/usr/bin/env nextflow 同很多Unix-like脚本一样，第一行叫做“shebang” (Hash bang)，出现在脚本第一行并以#!开头。它告诉系统用什么环境软件去解析这个脚本，当它存在并且脚本可执行的时候，我们可以通过直接调用该脚本来运行程序。以下为示例：
通过Interpreter来调用脚本（shebang不存在时也可使用）：  nextflow run ./main.nf 直接调用（需要shebang）：  chmod u+x .</description>
    </item>
    
    <item>
      <title>基于Nextflow的宏基因组有参分析-I 安装Nextflow</title>
      <link>/post/2019-09-27-nextflow-install/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-27-nextflow-install/</guid>
      <description> 开一个坑 本系列介绍如何搭建基于Nextflow的宏基因组有参分析流程。在此之前我曾经使用bpipe和snakemake写过这个流程。在我们实验室中所有的有参分析都是这个pipeline跑出来的。个人感觉bpipe用起来其实很顺手自然，但是由于社区实在是不够活跃。snakemake基于python而且有很多开发者支持，所以为了学习snakemake，我将之前的流程重新写成了snakemake。但是后来渐渐发现make的这种从后向前结构很别扭，基于文件名构建DAG也是缺乏灵活性。
Nextflow作为后起之秀迅速成长，拥有很好的社区支持（google group，gitter），原生对云环境（AWS，google cloud）支持，拥有大量优秀成熟流程nf-core。最近的一次更新推出了模块化的DSL 2架构，构建可重复利用的流程未来可期。为了更好地学习Nextflow，我将把之前的shotgunMetagenomics流程重写，并把过程记录下来，也希望可以帮助到更多人。
 流程介绍 这个流程可以参见snakemake生成的DAG:
步骤为：
合并相同样本 去接头 去宿主DNA 使用Profiling工具（kraken2，MetaPhlAn2） 按系统分类拆分 合并  ## 安装Nextflow
依赖：java &amp;gt;= 1.8
稳定版本安装：
curl -s https://get.nextflow.io | bash Github开发版本安装，支持DSL2语法：
git clone https://github.com/nextflow-io/nextflow.git cd nextflow make compile make pack 最后的Nextflow在build/releases/nextflow-19.09.0-edge-all，可以重新命名为nextflow并放在$PATH中。
export PATH=$PATH:/PATH/TO/REPO/build/releases/ (未完)
 </description>
    </item>
    
    <item>
      <title>启动Conda环境时自动更改环境变量</title>
      <link>/post/2019-08-17-conda-activate/</link>
      <pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-17-conda-activate/</guid>
      <description>这是一个神奇的路径 我们在服务器上用conda创建了一个叫metagenomics的环境，希望将常用分析软件安装在该环境。其中一些软件需要使用新版本的gcc编译（如：kraken2）。安装的准备套路应该如下：
export PATH=/opt/gcc-4.9.3/bin/:$PATH export LD_LIBRARY_PATH=/opt/gcc-4.9.3/lib64/:$LD_LIBRARY_PATH 那么如果我希望以上环境变量仅对我特定的Conda环境起效（metagenomics）应该怎么办呢？我们需要像.bashrc (.bash_profile)和.bash_logout这样在环境activate和deactivate时自动执行的脚本。在Conda中，我们需要如下目录：
ANACONDA_PATH/envs/ENV_PATH/etc/conda/ ├── activate.d │ └── env_vars.sh ## &amp;lt;- sourced when you do `conda activate` └── deactivate.d └── env_vars.sh ## &amp;lt;- sourced when you do `conda deactivate` 剩下的就是shell脚本的编写了，在下面的例子中，我们创建了一个叫做metagenomics的环境，并在其中安装了metaphlan2，kraken2，strainphlan。
activate.d/env_vars.sh #!/bin/sh ## CANU export OLD_PATH=$PATH export PATH=$(echo $PATH | sed &amp;#39;s;/mnt/software/unstowable/anaconda/envs/metagenomics/bin:;/mnt/software/unstowable/anaconda/envs/metagenomics/bin:/mnt/software/unstowable/biobakery-metaphlan2-26610e07f840:/mnt/software/unstowable/biobakery-metaphlan2-26610e07f840/utils/:/mnt/software/unstowable/biobakery-metaphlan2-26610e07f840/strainphlan_src/:;&amp;#39;) export R_LIBS=/mnt/software/unstowable/anaconda/envs/metagenomics/lib/R/library export BOWTIE2_INDEXES=/mnt/genomeDB/misc/softwareDB/metaphlan/huttenhower.sph.harvard.edu/metaphlan/bowtie2db/ ## Kraken2 export OLD_LD_LIBRARY_PATH=$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/opt/gcc-4.9.3/lib64:$LD_LIBRARY_PATH  deactivate.d/env_vars.sh #!/bin/sh export PATH=$OLD_PATH export R_LIBS= export BOWTIE2_INDEXES= export LD_LIBRARY_PATH=$OLD_LD_LIBRARY_PATH 在旧版的Anaconda中，source deactivate默认会把PATH变量中的第一个目录删掉，所以上面使用了sed来把新目录插到第二个的位置。现在的conda版本(使用conda deactivate)好像不存在这样的问题了。</description>
    </item>
    
    <item>
      <title>Emacs远程连接R</title>
      <link>/post/2018-11-25-emacs-remote-r/</link>
      <pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-11-25-emacs-remote-r/</guid>
      <description> 本地的R脚本，远程执行 从事生信工作，已经形成了对R语言的重度依赖，Rstudio作为专注于R的IDE拥有强大的优势，但是我还是更习惯于Emacs的ESS的操作。其实之前就遇到过一个挺有意思的问题，如何用本地的Emacs去编辑并执行服务器上的R脚本。一个很显而易见的方法就是用Emacs的Tramp SSH连接到服务器，打开R脚本，这个时候ESS会自动使用远程服务器的R来运行。但是问题来了，在画图的时候这个远程的R进程并不能通过X11把图传回，所以会自动存储为Rplot.pdf于工作目录。Google了很久，找到了以解决方案：
打开本地shell/eshell (M-x shell/eshell) 在打开的shell中连接远程服务器（ssh -X user@remote.machine.ip 或 ssh -Y）并运行R 使用ESS remote（M-x ess-remote） 打开本地或远程（Tramp mode）的R脚本 使用C-c C-n（在CUA mode关闭情况下可使用C-Enter）或C-c C-c逐行或区域执行  在上述过程中，顺序其实不是很重要，唯一需要注意的是如果R脚本在服务器上，一定要先开启一个本地的shell，要不然Emacs会默认使用远程机器的shell。
为了简化此过程，我写了一个函数并把它定义给快捷键C-c C-r（写在init.el或.emacs文件中）：
(defun spawn-ess-remote (login) &amp;quot;connect to remote server and open ssh&amp;quot; (interactive &amp;quot;sUser login (uname@server.ip): &amp;quot;) (pop-to-buffer (get-buffer-create (generate-new-buffer-name &amp;quot;Remote-R&amp;quot;))) (shell (current-buffer)) (process-send-string nil (format&amp;quot;ssh -Y %s \n&amp;quot; login) ) (process-send-string nil &amp;quot;R\n&amp;quot;) (ess-remote nil &amp;quot;R&amp;quot;)) (global-set-key (kbd &amp;quot;C-c C-r&amp;quot;) &amp;#39;spawn-ess-remote)   </description>
    </item>
    
    <item>
      <title>Frequently Googled Questions</title>
      <link>/post/2018-09-05-bioinfo-cheatsheet/</link>
      <pubDate>Wed, 05 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-09-05-bioinfo-cheatsheet/</guid>
      <description> Linux  Extract (decompress) with a file pattern (wildcard) ref  tar -xf MyTar.tar --wildcards &amp;quot;*.jpg&amp;quot;  ggplot2  rotate axis label ref  p + theme(axis.text.x = element_text(angle = 90, hjust = 1))  remove legend title ref  # Remove title for fill legend p + guides(fill=guide_legend(title=NULL)) # Remove title for all legends p + theme(legend.title=element_blank())  legends in multiple columns/rows ref A lot of interesting and useful things in this ref overriding the alpha value customizing the legend text angle etc  p + guides(col = guide_legend(nrow = 8))  </description>
    </item>
    
    <item>
      <title>一（三）文你读不懂PCA和PCoA</title>
      <link>/post/2018-04-39-r-pca_pcoa/</link>
      <pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-04-39-r-pca_pcoa/</guid>
      <description>三文读懂PCA和PCoA？ 今天看到金唯智公众号的推文《三文读懂PCA和PCoA》（《一》,《二》,《三》)。作者以平民化的语言，剔除数学术语，介绍了二者的区别，是很好的尝试，但是文中提出的很多关键性总结，都存在着明显的漏洞。
特别指出在第二篇文章中：
“PCA基于物种丰度矩阵就意味着PCA分析的矩阵维度是就等于物种数目。换句话说，你要分析的样本如果要做PCA分析，那么一般来说有多少个物种就有多少个维度”。  既然说到丰度，那有一个很容易忽视的点，就是所有物种丰度相加为常数（1或100%），所以说数据的维数其实是物种数-1。而相对丰度其实并不在传统意义的欧式空间中（参见Aitchison的《The Statistical Analysis of Compositional Data》）。PCA涉及到的变换其实是会保持数据点之间欧式距离不变（考虑所有PC的话），那么PCA分析是否适用于丰度（成分）数据，是一个存在争议的课题（参见Aitchison的《Principal Component Analysis of Compositional Data》）。所以在成分数据（测序数据，特别是microbiome）数据的时候，我们常采用一些生态距离，然后做PCoA。
“同样的道理，PCoA基于样本间的距离矩阵就意味着PCoA分析的矩阵维度与样本数目相关。如果你要分析的样本做PCoA分析的话，那么一般来说有n个样本就至多有n-1个维度”。  这是一个n（样本数目）和p（维度）的问题，维度就是维度，p就是p，不能混淆。
“多数情况下，我们在做降维处理的时候，期望维数越低越好，这样我们就可以最大程度地保真原始数据”  天下没有免费的午餐，维数越低，保真度自然越低。而PCA、PCoA所做的是在低维空间中尽量多的保存数据之间的差异。
“如果样本数目比较多，而物种数目比较少，那肯定首选PCA；如果样本数目比较少，而物种数目比较多，那肯定首选PCoA”  这是一个很有意思的问题，其实PCoA和PCA的结果取决于PCoA的实现，但是直觉上想，既然PCA的变换会保存数据点间的欧氏距离，那么它和基于欧式距离的PCoA有什么区别呢？
下面做一个实验，我们用两组数据（样本数目&amp;gt;维度，纬度&amp;gt;样本数目）来看看R中常见的PCA和PCoA实现的结果有何不同。
iris：样本数目&amp;gt;维度 ## Low dimensional data (n&amp;gt;&amp;gt;p) data(iris) par(mfrow=c(2,2),cex=0.7, pch=19) ## PCA pca &amp;lt;- prcomp(iris[,-5]) plot(pca$x[,1:2], col=iris[,5], xlab=&amp;#39;PC1&amp;#39;,ylab=&amp;#39;PC2&amp;#39;, main=&amp;#39;PCA&amp;#39;) ## PCoA pcoa &amp;lt;- cmdscale(dist(iris[,-5], method = &amp;quot;euclidean&amp;quot;)) plot(pcoa, xlab=&amp;#39;MDS1&amp;#39;,ylab=&amp;#39;MDS2&amp;#39;, col=iris[,5], main=&amp;#39;PCoA&amp;#39;) ## pairwise distances plot(as.vector(dist(pca$x[,1:2])), as.vector(dist(pcoa)), xlab=&amp;#39;PCA&amp;#39;, ylab=&amp;#39;PCoA&amp;#39;, main=&amp;#39;Pairwise distances&amp;#39;, pch=19, col=rgb(0,0,0,0.3), cex=0.5) plot(as.vector(dist(pca$x[,1:2])) - as.</description>
    </item>
    
    <item>
      <title>Generating GraPhlAn-like microbiome visualization using ggtree and microbiomeViz</title>
      <link>/post/2018-04-20-r-microbiomeviz_example/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-04-20-r-microbiomeviz_example/</guid>
      <description>Data downloading and preprocessing Now let’s try to generate a figure like this:
Sample processed by MetaPhlAn:
https://bitbucket.org/nsegata/metaphlan/wiki/profiled_samples.tar.bz2
Script to merge samples:
https://bitbucket.org/nsegata/metaphlan/raw/2f1b17a1f4e9775fe1ce42c8481279a5e69f291f/utils/merge_metaphlan_tables.py
Merge files into a single table
python merge_metaphlan_tables.py profiled_samples/*.txt &amp;gt; profiled_samples/merged_abundance_table.txt  Now switch to R Load data and library df &amp;lt;- read.table(&amp;quot;~/Downloads/profiled_samples/merged_abundance_table.txt&amp;quot;, head=TRUE, stringsAsFactors = FALSE) df &amp;lt;- df[,-ncol(df)] ## Use row means as a proxy for node size dat &amp;lt;- data.frame(V1=df[,1], V2=rowMeans(df[,-1]), stringsAsFactors = FALSE) library(microbiomeViz) ## Warning: replacing previous import &amp;#39;ape::rotate&amp;#39; by &amp;#39;ggtree::rotate&amp;#39; when ## loading &amp;#39;microbiomeViz&amp;#39;  Parse data and create a backbone tr &amp;lt;- parseMetaphlanTSV(dat, node.</description>
    </item>
    
    <item>
      <title>Y叔的新包base2grob</title>
      <link>/post/2018-03-29-r-base2grob_igraph/</link>
      <pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-03-29-r-base2grob_igraph/</guid>
      <description>测试了一下Y叔的新包base2grob。这个包提供了base2grob函数，可以把base plot转换成grob对象。对于我来说最大的方便就是可以把一些igraph画出的网络图通过cowplot和其他ggplot做出的图自由组合。
library(base2grob) library(igraph) ## ## Attaching package: &amp;#39;igraph&amp;#39; ## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## decompose, spectrum ## The following object is masked from &amp;#39;package:base&amp;#39;: ## ## union library(cowplot) ## Loading required package: ggplot2 ## ## Attaching package: &amp;#39;cowplot&amp;#39; ## The following object is masked from &amp;#39;package:ggplot2&amp;#39;: ## ## ggsave g1 &amp;lt;- erdos.renyi.game(10, 0.2) g2 &amp;lt;- erdos.renyi.game(10, 0.5) E(g1)$width &amp;lt;- E(g2)$width &amp;lt;- 5 E(g1)$label.cex &amp;lt;- E(g2)$label.</description>
    </item>
    
    <item>
      <title>microbiomeViz--斜体显示taxon name</title>
      <link>/post/2018-02-01-r-metagenomeviz-italictaxon/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-02-01-r-metagenomeviz-italictaxon/</guid>
      <description> 微生物种名属名经常需要斜体表示，现在microbiomeViz添加了一个生成expression的函数，可以直接将一个短语中的一个（个人认为很少会遇到画图时axis label涉及多个物种）指定名字变成斜体。
library(microbiomeViz) ## Warning: replacing previous import &amp;#39;ape::rotate&amp;#39; by &amp;#39;ggtree::rotate&amp;#39; when ## loading &amp;#39;microbiomeViz&amp;#39; library(ggplot2) ggplot(SRS014459_Stool_profile, aes(x=V2)) + geom_histogram(col=&amp;#39;black&amp;#39;) + theme_bw() + theme(axis.title = element_text(size=23)) + labs(x=formatPhrase(&amp;quot;Hello E. coli! Good-bye!&amp;quot;,&amp;quot;E. coli&amp;quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. </description>
    </item>
    
    <item>
      <title>microbiomeViz--人生第一个R包</title>
      <link>/post/2018-01-18-r-metagenomeviz/</link>
      <pubDate>Thu, 18 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-18-r-metagenomeviz/</guid>
      <description>为啥写这个 平日经常会分析shotgun宏基因组的数据，我们的pipeline使用MetaPhlAn，Kraken等profiler。这种数据经常会产生一个表格，如下：
download.file(&amp;quot;https://github.com/biobakery/biobakery/raw/master/demos/biobakery_demos/data/metaphlan2/output/SRS014459-Stool_profile.txt&amp;quot;, &amp;#39;~/Downloads/SRS014459-Stool_profile.txt&amp;#39;) knitr::kable(head(read.table(&amp;#39;~/Downloads/SRS014459-Stool_profile.txt&amp;#39;)))   V1 V2    k__Bacteria 100.00000  k__Bacteria|p__Firmicutes 64.91753  k__Bacteria|p__Bacteroidetes 35.08247  k__Bacteria|p__Firmicutes|c__Clostridia 64.91753  k__Bacteria|p__Bacteroidetes|c__Bacteroidia 35.08247  k__Bacteria|p__Firmicutes|c__Clostridia|o__Clostridiales 64.91753    第一列是分类信息注释，第二列是相对丰度（百分比）。在做这种图可视化方面，目前个人见过最强大的是GraPhlAn:
官网上相关的教程很详细，但是问题是，这个完全封闭的python程序，想要hack，还真的是挺难得。Krona可能是另一个选择，但是同样还是会有同样的问题。最近发布的R包Metacoder，画出的图个人真心不是很喜欢：
跟Y叔讨论了一下用ggtree实现像GraPhlAn那样图的可能性，得到了肯定的答复，于是开始自己造轮子。
 MicrobiomeViz–千里之行，始于足下 其实可以写一个简单的函数，但是还是想做一个拓展性更强的东西，所以就有了这个包（不断完善中）： https://github.com/lch14forever/microbiomeViz
 MetaPhlan结果的parser 安装 devtools::install_github(&amp;quot;lch14forever/microbiomeViz&amp;quot;, dependencies = T) ## Skipping install of &amp;#39;microbiomeViz&amp;#39; from a github remote, the SHA1 (3db1d3a6) has not changed since last install. ## Use `force = TRUE` to force installation  使用 目前有三个函数：</description>
    </item>
    
    <item>
      <title>成分之咒与ReBoot算法</title>
      <link>/post/2018-01-12-r-compositional/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-12-r-compositional/</guid>
      <description>写在前面 本文的大部分内容都来自我们之前的一篇综述文章Li et al, 2016。
 微生物组测序与成分数据 微生物组测序数据的获得其实有很多抽样过程（Sampling process）存在，比如说，粪便微生物组其实是对肠道微生物的一次抽样，测序的过程也是对所有DNA分子的一次抽样。最后，我们得到的OTU表中的OTU read count与测序深度相关，很显然测序深度是一个技术上的干扰因子（confounder），所以我们要对OTU表进行标准化（Normalization）。最简单的标准化方法，就是用read count除以样本内所有OTU的read count之和，获得每个OTU的相对丰度（relative abundance）。这种只有相对丰度的数据（相加总数为常数1或100）被称为成分数据（Compositional data）。 你可能注意到，作为成分数据一种的微生物组的数据中缺乏一个重要的信息–微生物总量（total abundance）。
 成分之咒（Curse of compositionality） （一些推导的废话可以跳过） 成分数据会对一些列统计分析产生影响，最典型的就是计算相关系数（correlation）。其中Pearson相关系数\(\rho_{X,Y}\)是由协方差矩阵算出来的，对于两个OTU的数量（由随机变量\(X\)，\(Y\)表示），相关系数可以按下面公式计算（详细解释见维基百科）：
\[COV(X, Y)=E[(X-E[X])(Y-E[Y])]\]
\[\rho_{X,Y}=\frac{COV(X,Y)}{\sqrt{COV(X,X)\times COV(Y,Y)}}\]
其中\(COV(X,X)\)其实就是\(X\)的方差。其中根据协方差的性质我们可以得出成分数据的协方差性质：
\[\sum_{i=1}^px_i=1 \Rightarrow \sum_{i=1,i\neq r}^pCOV(x_i, x_r)=-COV(x_r, x_r)\]
我们知道方差（\(COV(x_r, x_r)\)）一定为正值，所以成分数据的协方差（同理相关系数）天然趋向于负值！
 天然的负相关 上面的公式推导，其实简单的想一想，因为相加为1，所以一个OTU相对丰度增加，其他OTU必然减少，所以本来不相关的OTU数量在转化为成分数据后也会有负相关的趋势，也就是我们常见的微生物组领域所说的Compositional effect或Compositional bias。以下为简单的一个实验，Species 1和Speacies 2本来不相关，但是其相对丰度负相关：
## 加载r包 library(ggplot2) library(reshape2) library(cowplot) ## ggplot主题配置 figtheme &amp;lt;- theme_bw() + theme(text = element_text(size=10,face=&amp;#39;bold&amp;#39;),panel.border = element_rect(colour = &amp;quot;black&amp;quot;,size=2)) theme_set(figtheme) ## 随机产生独立的5个OTU绝对数量 means &amp;lt;- c(400,300,95,90,85) data &amp;lt;- sapply(means, function(x) rnorm(100, x, x*0.</description>
    </item>
    
    <item>
      <title>ggplot扩展学习笔记--逐行解析Y叔的&#34;geom_ord_ellipse.R&#34;</title>
      <link>/post/2018-01-09-r-ordellipsecode/</link>
      <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-09-r-ordellipsecode/</guid>
      <description>介绍 看到Y叔为ggord做的添加置信椭圆的geom_ord_ellipse.R（用法见上一篇文章），决定学习一点ggplot图形的语言，对于初学者最好的方法就是照葫芦画瓢，而Y叔的代码自然是最好的模板。我对Y叔的代码进行了逐行的分析，希望以后有需要可以套用。
以下为geom_ord_ellipse.R代码。这个图层的代码其实很短，很简洁，但是如果想要透彻理解还是需要下些功夫的。
##&amp;#39; add confidence ellipse to ordinary plot produced by ggord ##&amp;#39; ##&amp;#39; ##&amp;#39; @title geom_ord_ellipse ##&amp;#39; @param mapping aes mapping ##&amp;#39; @param ellipse_pro confidence value for the ellipse ##&amp;#39; @param fill color to fill the ellipse, NA by default ##&amp;#39; @param ... additional parameters ##&amp;#39; @return ggplot layer ##&amp;#39; @importFrom ggplot2 aes_ ##&amp;#39; @importFrom ggplot2 layer ##&amp;#39; @importFrom utils modifyList ##&amp;#39; @export ##&amp;#39; @author Guangchuang Yu ##&amp;#39; @references \url{http://lchblogs.</description>
    </item>
    
    <item>
      <title>Useful commands for Docker</title>
      <link>/post/2017-12-31-bioinfo-docker_notes/</link>
      <pubDate>Sun, 31 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-12-31-bioinfo-docker_notes/</guid>
      <description> 改变Docker容器存储路径 Linux下Docker的默认存储目录为/var/lib/docker。显然这个目录属于root分区（如果安装时进行过分区的话）。而一般这个分区会较小，我们更希望把容器和镜像存储于/home下。我根据这篇博客进行了更改https://sanenthusiast.com/change-default-image-container-location-docker/
 Docker常用操作  加载镜像：  gunzip -c [docker.img.tar.gz] | docker load docker pull [docker_image_name] cat [docker_file] | docker build -t [image_name] -  启动容器  # 基本命令 docker run -it [image_name] # 退出时删除容器，加载host的文件路径 docker run -it --rm -v /host/directory:/container/directory [image_name]  attach  docker attach [container_name/container_hash_ID]  detach（容器在后台运行）：  Ctrl+p Ctrl+q
 列出所有容器（包括未在运行中的）  docker ps -a docker rm [container] # 删除容器  列出所有镜像  docker images docker rmi [image] # 删除镜像  host和容器文件转移  docker cp [source] [target] # use container hash id or name   Commit  docker commit [container_name/container_hash_ID] [image_ID/image_name:image_tag]  </description>
    </item>
    
    <item>
      <title>Add confidence ellipse to LDA ordination plot II</title>
      <link>/post/2017-12-29-r-addconfellipse/</link>
      <pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-12-29-r-addconfellipse/</guid>
      <description>为排序图添加置信范围曲线 上一篇文章我们利用ggord的源代码改写了一个为LDA排序图添加置信曲线的函数，现在Y叔已经把它改写成了一个geom添加在了他的yyplot包中。y叔扩展了它的功能，现在支持ggord中的其他排序图（我没有进行全面的测试）。
library(ggord) library(yyplot) library(MASS) LDA ord &amp;lt;- lda(Species ~ ., iris, prior = rep(1, 3)/3) ggord(ord, iris$Species) + geom_ord_ellipse(lty=2)  PCA ord &amp;lt;- prcomp(iris[, 1:4]) ggord(ord, iris$Species) + geom_ord_ellipse(lty=2)  MDS library(vegan) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.4-4 ord &amp;lt;- metaMDS(iris[, 1:4]) ## Run 0 stress 0.03775523 ## Run 1 stress 0.05879963 ## Run 2 stress 0.05771902 ## Run 3 stress 0.</description>
    </item>
    
    <item>
      <title>Add confidence ellipse to LDA ordination plot</title>
      <link>/post/2017-12-22-r-addconfellipselda/</link>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-12-22-r-addconfellipselda/</guid>
      <description>Use ggord to plot LDA ordination plot Installation devtools::install_github(&amp;#39;fawda123/ggord&amp;#39;)  Basic LDA ordination biplot library(MASS) ord &amp;lt;- lda(Species ~ ., iris, prior = rep(1, 3)/3) library(ggord) p &amp;lt;- ggord(ord, iris$Species) p   A function to compute confidence ellipse get_lda_ell &amp;lt;- function(ord_in, grp_in, ellipse_pro = 0.97){ ## adapted from https://github.com/fawda123/ggord/blob/master/R/ggord.R require(plyr) axes = c(&amp;#39;LD1&amp;#39;, &amp;#39;LD2&amp;#39;) obs &amp;lt;- data.frame(predict(ord_in)$x[, axes]) obs$Groups &amp;lt;- grp_in names(obs)[1:2] &amp;lt;- c(&amp;#39;one&amp;#39;, &amp;#39;two&amp;#39;) theta &amp;lt;- c(seq(-pi, pi, length = 50), seq(pi, -pi, length = 50)) circle &amp;lt;- cbind(cos(theta), sin(theta)) ell &amp;lt;- ddply(obs, &amp;#39;Groups&amp;#39;, function(x) { if(nrow(x) &amp;lt;= 2) { return(NULL) } sigma &amp;lt;- var(cbind(x$one, x$two)) mu &amp;lt;- c(mean(x$one), mean(x$two)) ed &amp;lt;- sqrt(qchisq(ellipse_pro, df = 2)) data.</description>
    </item>
    
    <item>
      <title>有了绝对定量就完了？</title>
      <link>/post/2017-11-18-r-absolute/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-18-r-absolute/</guid>
      <description>微生物组研究走向绝对定量 Jeroen Raes研究组上周在Nature发表文章，使用Flow cytometry估算粪便中的微生物细胞数量，配合16S测序算出的相对丰度，估算出OTU的绝对数量（absolute abundance）。文章很多结论都印证了直接应用相对丰度进行分析时，我们所看到的很多现象是成分数据性质的假象（可以想象，由于相对丰度在每一个样本中相加为1或100，一个OTU相对丰度增加必将引起其他OTU相对丰度减少，所谓的compositional bias）。特别针对于计算两个OTU的相关系数，当OTU分布不均匀时，很容易看到负相关的OTU–而这仅仅是因为它们受到相加为常数的限制而已。另一个典型例子就是主成分分析（PCA），PCA意在保持欧式距离不变的情况下对数据进行变换，但是相对丰度其实不在欧式空间中（可以参考:J. Aitchison, The Statistical Analysis of Compositional Data, 1986.），这就是为什么在微生物组的研究中更多采用生态学的距离（如，Bray-Curtis distance）来计算\(\beta\) -diversity，然后进行基于距离矩阵的分析（PCoA）。
似乎文章的take home message很简单了，微生物组的研究，我们应该使用类似的方法进行绝对定量。可是仔细想一想，文章指出粪便微生物总量的个体差异可以达到10倍之多，这样大的差异，如果某种微生物在个体之间差异很小，转换成绝对数量之后，个体间的差异将受制于微生物总量。
 使用Flow cytometry测定的肠道微生物总量的variation有多大？ 下载Nature文章中的Supplementary Table，这里并不不需要购买文章阅读权限，其中表6是Flow cytometry的数据。
加载R包，ggplot2主题 library(tidyverse) ## ── Attaching packages ─────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.3.4 ✔ dplyr 0.7.4 ## ✔ tidyr 0.7.2 ✔ stringr 1.2.0 ## ✔ readr 1.1.1 ✔ forcats 0.2.0 ## ── Conflicts ────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(data.</description>
    </item>
    
    <item>
      <title>Generalized Lotka-Volterra model</title>
      <link>/post/2017-11-15-r-glv/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-15-r-glv/</guid>
      <description>The model \(\frac{dx_i}{dt}= ax_i+\sum_{j=1}^pb_{ij}x_ix_j\)
## library library(&amp;#39;deSolve&amp;#39;) ## GLV function lvm &amp;lt;- function(t, x, params){ with(as.list(params, c(x)), { dx &amp;lt;- alpha * x + x * (beta %*% x) list(dx) }) } ## numerical integration n.integrate &amp;lt;- function(time, init.x, model, params){ as.data.frame(ode(init.x, time, model, params)) } ## normalizations TSS &amp;lt;- function(x){ apply(x, 1, function(x)x/sum(x)) }  GLV with 3 species alpha &amp;lt;- c(0.1, 0.2, 0.3) beta &amp;lt;- t(matrix(c(-0.5, -0.3 , 0, 0.</description>
    </item>
    
    <item>
      <title>从curatedMetagenoimcData提取健康样本的微生物组</title>
      <link>/post/2017-11-07-r-curatedmetagenomicdata_retrievedata/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-07-r-curatedmetagenomicdata_retrievedata/</guid>
      <description>简介 有人问我，这个curatedMetagenomicData有什么作用？对于我们这些research parasite(出处请见NEJM社论)来说，这样庞大的数据集当然是用来挖信息或者是测试算法了：
作为validation cohort来验证已发现的结论。如我们的这篇文章都使用了2014年Oh et al发表于Nature的数据来佐证我们发现的Staphylococcus特异性突变。
 用来测试新方法，发现新的生物问题。如这篇文章中我们也用了Oh et al的数据来观测Malessezia在人体皮肤的分布。
 因为数据中有大量健康人的微生物组（不同研究中的control），我们也可以用它们补充我们的对照组（当然在机器学习中要注意数据不平衡问题）。
  当然最基础的操作，就是要从数据库中提取数据。
 演示 我们从curatedMetagenomicData中提取健康人的粪便及皮肤微生物组。
加载R package suppressMessages(library(curatedMetagenomicData)) suppressMessages(library(foreach))  下载并合并所有粪便、皮肤数据（下载数据缓存于~/.ExperimentHub中，第二次以后会直接读取缓存） ## download all stool samples stoolData &amp;lt;- curatedMetagenomicData(&amp;quot;*.metaphlan_bugs_list.stool&amp;quot;, dryrun=FALSE) ## Warning in strptime(x, fmt, tz = &amp;quot;GMT&amp;quot;): unknown timezone &amp;#39;zone/tz/2017c. ## 1.0/zoneinfo/Asia/Singapore&amp;#39; skinData &amp;lt;- curatedMetagenomicData(&amp;quot;*.metaphlan_bugs_list.skin&amp;quot;, dryrun=FALSE) ## merge stoolDataMerged &amp;lt;- mergeData(stoolData) skinDataMerged &amp;lt;- mergeData(skinData)  验证元数据与样本对应关系 table(rownames(pData(stoolDataMerged)) == colnames(exprs(stoolDataMerged))) ## ## TRUE ## 4810 table(rownames(pData(skinDataMerged)) == colnames(exprs(skinDataMerged))) ## ## TRUE ## 466  通过元数据来提取健康样本的数据 顺便过滤掉整行、整列为0的数据</description>
    </item>
    
    <item>
      <title>A collection of useful tools (Keep updating)</title>
      <link>/post/2017-11-04-bioinfo-bioinforcollections/</link>
      <pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-04-bioinfo-bioinforcollections/</guid>
      <description> General bioinformatics  Applied genomics course from Michael Schatz https://github.com/schatzlab/appliedgenomics Data science from Rafael A. Irizarry (built with R bookdown) https://rafalab.github.io/dsbook/   Nanopore seqeuncing data analysis  Assembly best practice QC Adaptor removal with Porechop (Minimap2 + Miniasm ) or Canu + Racon + Nanopolish    Metagenomics  Coming soon …   Single cell analysis  Tool collection for SC data https://github.com/seandavi/awesome-single-cell   </description>
    </item>
    
    <item>
      <title>桑基图（Sankey diagram）II</title>
      <link>/post/2017-11-02-r-sankeyplot2/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-11-02-r-sankeyplot2/</guid>
      <description>使用plotly 预处理 taxtab &amp;lt;- read.table(&amp;#39;SRS014476-Supragingival_plaque_profile.txt&amp;#39;, sep=&amp;#39;\t&amp;#39;, stringsAsFactors=FALSE) second_last &amp;lt;- function(x) ifelse(length(x)&amp;gt;1, x[length(x)-1], NA) taxtab &amp;lt;- taxtab[-grep(&amp;#39;t__.*unclassified&amp;#39;, taxtab$V1), ] ## remove strain level tax_split &amp;lt;- strsplit(taxtab$V1, &amp;#39;\\|&amp;#39;) ## split into different taxonomy levels target &amp;lt;- sapply(tax_split, tail, n=1) ## target node mapping &amp;lt;- data.frame(id=0:(length(target)-1), row.names=target) target_id &amp;lt;- mapping$id source &amp;lt;- sapply(tax_split, second_last) ## source node source_id &amp;lt;- mapping[source,] value &amp;lt;- taxtab$V2 ## width of flow links &amp;lt;- data.</description>
    </item>
    
    <item>
      <title>超大宏基因组数据集CuratedMetagenomicData</title>
      <link>/post/2017-10-30-r-curatedmetagenomicdata/</link>
      <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-10-30-r-curatedmetagenomicdata/</guid>
      <description>简介 CuratedMetagenomicData(https://waldronlab.github.io/curatedMetagenomicData/)的目标是用标准化的流程（MetaPhlan2、HUMAnN2）分析已发表的宏基因组数据并建立一个统一的数据集合。目前已经收录6000余个样本并在持续扩建当中。样本涵盖糖尿病、肥胖症、IBD等多种疾病，涉及皮肤、口腔、粪便等多处样本。
 完整帮助信息 https://bioconductor.org/packages/devel/data/experiment/vignettes/curatedMetagenomicData/inst/doc/curatedMetagenomicData.html
 安装 通过Bioconductor安装最新版本
## try http:// if https:// URLs are not supported source(&amp;quot;https://bioconductor.org/biocLite.R&amp;quot;) useDevel() biocLite(&amp;quot;curatedMetagenomicData&amp;quot;)  基本使用 调用
suppressPackageStartupMessages(library(curatedMetagenomicData)) 查看metadata
combined_metadata ## # A tibble: 6,058 x 80 ## dataset_name sampleID subjectID body_site antibiotics_current_use ## &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; ## 1 AsnicarF_2017 MV_FEI1_t1Q14 MV_FEI1 stool &amp;lt;NA&amp;gt; ## 2 AsnicarF_2017 MV_FEI2_t1Q14 MV_FEI2 stool &amp;lt;NA&amp;gt; ## 3 AsnicarF_2017 MV_FEI3_t1Q14 MV_FEI3 stool &amp;lt;NA&amp;gt; ## 4 AsnicarF_2017 MV_FEI4_t1Q14 MV_FEI4 stool &amp;lt;NA&amp;gt; ## 5 AsnicarF_2017 MV_FEI4_t2Q15 MV_FEI4 stool &amp;lt;NA&amp;gt; ## 6 AsnicarF_2017 MV_FEI5_t1Q14 MV_FEI5 stool &amp;lt;NA&amp;gt; ## 7 AsnicarF_2017 MV_FEI5_t2Q14 MV_FEI5 stool &amp;lt;NA&amp;gt; ## 8 AsnicarF_2017 MV_FEI5_t3Q15 MV_FEI5 stool &amp;lt;NA&amp;gt; ## 9 AsnicarF_2017 MV_FEM1_t1Q14 MV_FEM1 stool &amp;lt;NA&amp;gt; ## 10 AsnicarF_2017 MV_FEM2_t1Q14 MV_FEM2 stool &amp;lt;NA&amp;gt; ## # .</description>
    </item>
    
    <item>
      <title>桑基图（Sankey diagram）</title>
      <link>/post/2017-10-29-r-sankeyplot/</link>
      <pubDate>Sun, 29 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-10-29-r-sankeyplot/</guid>
      <description>简介 桑基图（Sankey diagram）是用于表示能量或信息流动的一种可视化方式，应用于微生物组数据，可以清晰展示各个taxonomy level之间物种相对丰度的流动。从Domain到Species，相邻两级之间分支的总宽度保持不变（能量守恒），如下图：  数据准备 此处我们使用Metaphlan2 tutorial中的数据，来源于牙龈菌斑的宏基因组测序：
download.file(&amp;#39;https://github.com/biobakery/biobakery/raw/master/demos/biobakery_demos/data/metaphlan2/output/SRS014476-Supragingival_plaque_profile.txt&amp;#39;, destfile = &amp;#39;SRS014476-Supragingival_plaque_profile.txt&amp;#39;) taxtab &amp;lt;- read.table(&amp;#39;SRS014476-Supragingival_plaque_profile.txt&amp;#39;, sep=&amp;#39;\t&amp;#39;, stringsAsFactors=FALSE)  加载networkdD3包 该包使用D3.js实现数据的交互式可视化，具体一些例子参见这里：
library(networkD3)  数据预处理 函数sankeyNetwork主要需要两个data frame: Links和Nodes。Links主要有连接的起点（source）和终点（target），似乎此处的起点和终点只能是从0开始的数字。Nodes如果不指定ID，默认行是按照0开始的数字排列。
second_last &amp;lt;- function(x) ifelse(length(x)&amp;gt;1, x[length(x)-1], NA) taxtab &amp;lt;- taxtab[-grep(&amp;#39;unclassified&amp;#39;, taxtab$V1), ] ## remove unlassified taxa tax_split &amp;lt;- strsplit(taxtab$V1, &amp;#39;\\|&amp;#39;) ## split into different taxonomy levels target &amp;lt;- sapply(tax_split, tail, n=1) ## target node mapping &amp;lt;- data.frame(id=0:(length(target)-1), row.names=target) target_id &amp;lt;- mapping$id source &amp;lt;- sapply(tax_split, second_last) ## source node source_id &amp;lt;- mapping[source,] value &amp;lt;- taxtab$V2 ## width of flow links &amp;lt;- data.</description>
    </item>
    
  </channel>
</rss>